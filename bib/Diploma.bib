
@inproceedings{yang_wider_2016,
	address = {Las Vegas, NV, USA},
	title = {{WIDER} {FACE}: {A} {Face} {Detection} {Benchmark}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {{WIDER} {FACE}},
	url = {http://ieeexplore.ieee.org/document/7780965/},
	doi = {10.1109/CVPR.2016.596},
	abstract = {Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset1, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated.},
	language = {en},
	urldate = {2021-03-13},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
	month = jun,
	year = {2016},
	pages = {5525--5533},
	annote = {Extracted Annotations (4/2/2021, 2:49:03 PM)
"Benchmark" (Yang et al 2016:5525)},
	file = {undefined - Extracted Annotations (422021, 24903 PM)Benchmark (Yang et al 20165525).md:/home/phyl/data/onedrive/obsidian/flop/undefined - Extracted Annotations (422021, 24903 PM)Benchmark (Yang et al 20165525).md:text/markdown;Yang et al. - 2016 - WIDER FACE A Face Detection Benchmark.pdf:/home/phyl/Zotero/storage/RDZ5DW48/Yang et al. - 2016 - WIDER FACE A Face Detection Benchmark.pdf:application/pdf;WIDER FACE\: A Face Detection Benchmark:/home/phyl/Zotero/storage/56Q7UBMU/CVPR.2016.596.pdf.pdf:application/pdf},
}

@misc{noauthor_face_2018,
	title = {Face {Detection} – {OpenCV}, {Dlib} and {Deep} {Learning} ( {C}++ / {Python} )},
	url = {https://learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/},
	abstract = {We will compare the various Face Detection methods in OpenCV and Dlib. We discuss OpenCV Haar and DNN based face Detectors and Dlib HoG and MMOD face detectors.},
	language = {en-US},
	urldate = {2021-03-29},
	month = oct,
	year = {2018},
	file = {Snapshot:/home/phyl/Zotero/storage/9SMSF44U/face-detection-opencv-dlib-and-deep-learning-c-python.html:text/html},
}

@article{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets conﬁrm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, SSD achieves 74.3\% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9\% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
	language = {en},
	urldate = {2021-10-04},
	journal = {arXiv:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	note = {arXiv: 1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
	annote = {Comment: ECCV 2016},
	file = {Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:/home/phyl/Zotero/storage/TTAW2WIY/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf},
}

@article{zhang_faceboxes_2018,
	title = {{FaceBoxes}: {A} {CPU} {Real}-time {Face} {Detector} with {High} {Accuracy}},
	shorttitle = {{FaceBoxes}},
	url = {http://arxiv.org/abs/1708.05234},
	abstract = {Although tremendous strides have been made in face detection, one of the remaining open challenges is to achieve real-time speed on the CPU as well as maintain high performance, since effective models for face detection tend to be computationally prohibitive. To address this challenge, we propose a novel face detector, named FaceBoxes, with superior performance on both speed and accuracy. Specifically, our method has a lightweight yet powerful network structure that consists of the Rapidly Digested Convolutional Layers (RDCL) and the Multiple Scale Convolutional Layers (MSCL). The RDCL is designed to enable FaceBoxes to achieve real-time speed on the CPU. The MSCL aims at enriching the receptive ﬁelds and discretizing anchors over different layers to handle faces of various scales. Besides, we propose a new anchor densiﬁcation strategy to make different types of anchors have the same density on the image, which signiﬁcantly improves the recall rate of small faces. As a consequence, the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images. Moreover, the speed of FaceBoxes is invariant to the number of faces. We comprehensively evaluate this method and present stateof-the-art detection performance on several face detection benchmark datasets, including the AFW, PASCAL face, and FDDB. Code is available at https://github.com/ sfzhang15/FaceBoxes.},
	language = {en},
	urldate = {2021-10-04},
	journal = {arXiv:1708.05234 [cs]},
	author = {Zhang, Shifeng and Zhu, Xiangyu and Lei, Zhen and Shi, Hailin and Wang, Xiaobo and Li, Stan Z.},
	month = dec,
	year = {2018},
	note = {arXiv: 1708.05234},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by IJCB 2017; Added references; Released codes},
	file = {Zhang et al. - 2018 - FaceBoxes A CPU Real-time Face Detector with High.pdf:/home/phyl/Zotero/storage/LU3PYERG/Zhang et al. - 2018 - FaceBoxes A CPU Real-time Face Detector with High.pdf:application/pdf;1-s2.0-S0925231219310719-main.pdf:/home/phyl/Zotero/storage/HZ5SNKV5/1-s2.0-S0925231219310719-main.pdf:application/pdf},
}

@article{zafeiriou_survey_2015,
	title = {A survey on face detection in the wild: {Past}, present and future},
	volume = {138},
	issn = {10773142},
	shorttitle = {A survey on face detection in the wild},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314215000727},
	doi = {10.1016/j.cviu.2015.03.015},
	abstract = {Face detection is one of the most studied topics in computer vision literature, not only because of the challenging nature of face as an object, but also due to the countless applications that require the application of face detection as a ﬁrst step. During the past 15 years, tremendous progress has been made due to the availability of data in unconstrained capture conditions (so-called ‘in-the-wild’) through the Internet, the effort made by the community to develop publicly available benchmarks, as well as the progress in the development of robust computer vision algorithms. In this paper, we survey the recent advances in real-world face detection techniques, beginning with the seminal Viola–Jones face detector methodology. These techniques are roughly categorized into two general schemes: rigid templates, learned mainly via boosting based methods or by the application of deep neural networks, and deformable models that describe the face by its parts. Representative methods will be described in detail, along with a few additional successful methods that we brieﬂy go through at the end. Finally, we survey the main databases used for the evaluation of face detection algorithms and recent benchmarking efforts, and discuss the future of face detection.},
	language = {en},
	urldate = {2021-10-04},
	journal = {Computer Vision and Image Understanding},
	author = {Zafeiriou, Stefanos and Zhang, Cha and Zhang, Zhengyou},
	month = sep,
	year = {2015},
	pages = {1--24},
	file = {Zafeiriou et al. - 2015 - A survey on face detection in the wild Past, pres.pdf:/home/phyl/Zotero/storage/XUGH3FGW/Zafeiriou et al. - 2015 - A survey on face detection in the wild Past, pres.pdf:application/pdf},
}

@article{putro_high_2021,
	title = {High {Performance} and {Efficient} {Real}-{Time} {Face} {Detector} on {Central} {Processing} {Unit} {Based} on {Convolutional} {Neural} {Network}},
	volume = {17},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/9187678/},
	doi = {10.1109/TII.2020.3022501},
	number = {7},
	urldate = {2021-10-17},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Putro, Muhamad Dwisnanto and Kurnianggoro, Laksono and Jo, Kang-Hyun},
	month = jul,
	year = {2021},
	pages = {4449--4457},
	file = {Putro et al_2021_High Performance and Efficient Real-Time Face Detector on Central Processing.pdf:/home/phyl/Zotero/storage/NXL2UY2T/Putro et al_2021_High Performance and Efficient Real-Time Face Detector on Central Processing.pdf:application/pdf},
}

@book{putro_real-time_2018,
	title = {Real-time {Face} {Tracking} for {Human}-{Robot} {Interaction}},
	author = {Putro, Muhamad Dwisnanto and Jo, Kang-Hyun},
	month = sep,
	year = {2018},
	doi = {10.1109/ICT-ROBOT.2018.8549902},
	note = {Pages: 4},
	file = {Putro_Jo_2018_Real-time Face Tracking for Human-Robot Interaction.pdf:/home/phyl/Zotero/storage/5G3R5U8S/Putro_Jo_2018_Real-time Face Tracking for Human-Robot Interaction.pdf:application/pdf},
}

@article{miklosi_utilization_2012,
	title = {On the {Utilization} of {Social} {Animals} as a {Model} for {Social} {Robotics}},
	volume = {3},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00075/abstract},
	doi = {10.3389/fpsyg.2012.00075},
	urldate = {2021-11-20},
	journal = {Frontiers in Psychology},
	author = {Miklósi, Ádám and Gácsi, Márta},
	year = {2012},
	file = {Miklósi_Gácsi_2012_On the Utilization of Social Animals as a Model for Social Robotics.pdf:/home/phyl/Zotero/storage/MXAM6QKE/Miklósi_Gácsi_2012_On the Utilization of Social Animals as a Model for Social Robotics.pdf:application/pdf;Full Text:/home/phyl/Zotero/storage/BP9R8FQK/Miklósi and Gácsi - 2012 - On the Utilization of Social Animals as a Model fo.pdf:application/pdf},
}

@inproceedings{koay_heyx0021_2013,
	address = {Singapore, Singapore},
	title = {Hey\&\#x0021; {There} is someone at your door. {A} hearing robot using visual communication signals of hearing dogs to communicate intent},
	isbn = {978-1-4673-5863-7},
	url = {http://ieeexplore.ieee.org/document/6602436/},
	doi = {10.1109/ALIFE.2013.6602436},
	urldate = {2021-11-20},
	booktitle = {2013 {IEEE} {Symposium} on {Artificial} {Life} ({ALife})},
	publisher = {IEEE},
	author = {Koay, K. L. and Lakatos, G. and Syrdal, D. S. and Gacsi, M. and Bereczky, B. and Dautenhahn, K. and Miklosi, A. and Walters, M. L.},
	month = apr,
	year = {2013},
	pages = {90--97},
	file = {Koay et al_2013_Hey&#x0021\; There is someone at your door.pdf:/home/phyl/Zotero/storage/LMMMMXBH/Koay et al_2013_Hey&#x0021\; There is someone at your door.pdf:application/pdf},
}

@article{lakatos_emotion_2014,
	title = {Emotion {Attribution} to a {Non}-{Humanoid} {Robot} in {Different} {Social} {Situations}},
	volume = {9},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0114207},
	doi = {10.1371/journal.pone.0114207},
	language = {en},
	number = {12},
	urldate = {2021-11-20},
	journal = {PLoS ONE},
	author = {Lakatos, Gabriella and Gácsi, Márta and Konok, Veronika and Brúder, Ildikó and Bereczky, Boróka and Korondi, Péter and Miklósi, Ádám},
	editor = {Urgesi, Cosimo},
	month = dec,
	year = {2014},
	pages = {e114207},
	file = {Lakatos et al_2014_Emotion Attribution to a Non-Humanoid Robot in Different Social Situations.pdf:/home/phyl/Zotero/storage/XB5ASJHK/Lakatos et al_2014_Emotion Attribution to a Non-Humanoid Robot in Different Social Situations.pdf:application/pdf;Full Text:/home/phyl/Zotero/storage/2SVP6C32/Lakatos et al. - 2014 - Emotion Attribution to a Non-Humanoid Robot in Dif.pdf:application/pdf},
}

@article{miklosi_ethorobotics_2017,
	title = {Ethorobotics: {A} {New} {Approach} to {Human}-{Robot} {Relationship}},
	volume = {8},
	issn = {1664-1078},
	shorttitle = {Ethorobotics},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2017.00958/full},
	doi = {10.3389/fpsyg.2017.00958},
	urldate = {2021-11-20},
	journal = {Frontiers in Psychology},
	author = {Miklósi, Ádám and Korondi, Péter and Matellán, Vicente and Gácsi, Márta},
	month = jun,
	year = {2017},
	pages = {958},
	file = {Miklósi et al_2017_Ethorobotics.pdf:/home/phyl/Zotero/storage/KEBNP27P/Miklósi et al_2017_Ethorobotics.pdf:application/pdf;Full Text:/home/phyl/Zotero/storage/RBTTY6YU/Miklósi et al. - 2017 - Ethorobotics A New Approach to Human-Robot Relati.pdf:application/pdf},
}

@book{institute_of_electrical_and_electronics_engineers_2013_2013,
	address = {Piscataway, NJ},
	title = {2013 {IEEE} {Symposium} on {Artificial} {Life} ({ALife} 2013): {Singapore}, 16 - 19 {April} 2013 ; [part of the 2013 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})]},
	isbn = {978-1-4673-5863-7 978-1-4673-5862-0},
	shorttitle = {2013 {IEEE} {Symposium} on {Artificial} {Life} ({ALife} 2013)},
	language = {eng},
	publisher = {IEEE},
	editor = {Institute of Electrical {and} Electronics Engineers and IEEE Computational Intelligence Society},
	year = {2013},
	annote = {Literaturangaben},
	file = {Institute of Electrical and Electronics Engineers_IEEE Computational Intelligence Society_2013_2013 IEEE Symposium on Artificial Life (ALife 2013).pdf:/home/phyl/Zotero/storage/9V4MH765/Institute of Electrical and Electronics Engineers_IEEE Computational Intelligence Society_2013_2013 IEEE Symposium on Artificial Life (ALife 2013).pdf:application/pdf},
}

@article{korcsok_artificial_2020,
	title = {Artificial sounds following biological rules: {A} novel approach for non-verbal communication in {HRI}},
	volume = {10},
	issn = {2045-2322},
	shorttitle = {Artificial sounds following biological rules},
	url = {http://www.nature.com/articles/s41598-020-63504-8},
	doi = {10.1038/s41598-020-63504-8},
	abstract = {Abstract
            Emotionally expressive non-verbal vocalizations can play a major role in human-robot interactions. Humans can assess the intensity and emotional valence of animal vocalizations based on simple acoustic features such as call length and fundamental frequency. These simple encoding rules are suggested to be general across terrestrial vertebrates. To test the degree of this generalizability, our aim was to synthesize a set of artificial sounds by systematically changing the call length and fundamental frequency, and examine how emotional valence and intensity is attributed to them by humans. Based on sine wave sounds, we generated sound samples in seven categories by increasing complexity via incorporating different characteristics of animal vocalizations. We used an online questionnaire to measure the perceived emotional valence and intensity of the sounds in a two-dimensional model of emotions. The results show that sounds with low fundamental frequency and shorter call lengths were considered to have a more positive valence, and samples with high fundamental frequency were rated as more intense across all categories, regardless of the sound complexity. We conclude that applying the basic rules of vocal emotion encoding can be a good starting point for the development of novel non-verbal vocalizations for artificial agents.},
	language = {en},
	number = {1},
	urldate = {2021-11-20},
	journal = {Scientific Reports},
	author = {Korcsok, Beáta and Faragó, Tamás and Ferdinandy, Bence and Miklósi, Ádám and Korondi, Péter and Gácsi, Márta},
	month = dec,
	year = {2020},
	pages = {7080},
	file = {Korcsok et al_2020_Artificial sounds following biological rules.pdf:/home/phyl/Zotero/storage/LK78R9SX/Korcsok et al_2020_Artificial sounds following biological rules.pdf:application/pdf;Full Text:/home/phyl/Zotero/storage/GY4J43JX/Korcsok et al. - 2020 - Artificial sounds following biological rules A no.pdf:application/pdf},
}

@article{coombs_robovac_nodate,
	title = {Robovac and the {Cat} {Will} {Get} {Along} {Famously}},
	abstract = {The first robot vacuumsmight be realized sooner if their designs sacrifice someefficiency for robustness achieved by persistence, and if they can expect reasonable cooperation from the household. In many houses, there are times wheneach roomis unoccupied long enough to permit its being vacuumed without disrupting normal family life. Sacrificing sophistication to achieve minimal competence earlier, RoboVacversion 0.0 might operate only in "vacuum-proofed" houses. Similarly, early versions of RoboVac need not sweep the room with maximumefficiency if the roomis vacant for reasonable stretches of time. Over time, RoboVacshould increase its efficiency by learning the house layout and the best times to clean each area.},
	language = {en},
	author = {Coombs, David},
	pages = {3},
	file = {Coombs - Robovac and the Cat Will Get Along Famously.pdf:/home/phyl/Zotero/storage/5Z6WCHH7/Coombs - Robovac and the Cat Will Get Along Famously.pdf:application/pdf},
}

@book{engelberger_robotics_1989,
	address = {London},
	title = {Robotics in service},
	isbn = {978-1-85091-358-0},
	language = {eng},
	publisher = {Kogan Page},
	author = {Engelberger, Joseph F.},
	year = {1989},
}

@article{brittain_autonomous_nodate,
	title = {Autonomous {Vacuum} {Cleaners} {Must} {Be} {Bayesian}},
	abstract = {Due to limited power supplies, finite dust storage, poor location sensing, and random failures caused by sucking up numeroussmall objects, autonomous vacuumcleaners require accurate and timely information about environmental parameters and the state of internal components. The best methods for providing such estimates are Bayesian. The recent advent of compact, efficient probabilistic representations has made the implementation of these methods not only more tractable, but has also extended the range and scope of architectures in which they can be implemented. Weexplore the anomalies of the vacuuming domain and illustrate how Bayesian techniques can be applied to reduce their effects on agent performance.},
	language = {en},
	author = {Brittain, R Wade and D'Ambrosio, Bruce},
	pages = {4},
	file = {Brittain and D'Ambrosio - Autonomous Vacuum Cleaners Must Be Bayesian.pdf:/home/phyl/Zotero/storage/HZE5TJGM/Brittain and D'Ambrosio - Autonomous Vacuum Cleaners Must Be Bayesian.pdf:application/pdf},
}

@techreport{brutzman_beyond_1993,
	title = {Beyond intelligent vacuum cleaners},
	institution = {NAVAL POSTGRADUATE SCHOOL MONTEREY CA},
	author = {Brutzman, Don},
	year = {1993},
	file = {Brutzman_1993_Beyond intelligent vacuum cleaners.pdf:/home/phyl/Zotero/storage/DSQM2GZL/Brutzman_1993_Beyond intelligent vacuum cleaners.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/ZIUC6BGU/ADA280565.html:text/html},
}

@article{fiorini_cleaning_2000,
	title = {Cleaning and household robots: {A} technology survey},
	volume = {9},
	shorttitle = {Cleaning and household robots},
	number = {3},
	journal = {Autonomous robots},
	author = {Fiorini, Paolo and Prassler, Erwin},
	year = {2000},
	note = {Publisher: Springer},
	pages = {227--235},
	file = {Fiorini_Prassler_2000_Cleaning and household robots.pdf:/home/phyl/Zotero/storage/LUPCS7YU/Fiorini_Prassler_2000_Cleaning and household robots.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/GXGESPZ2/A1008954632763.html:text/html},
}

@inproceedings{brittain_autonomous_1993,
	title = {Autonomous vacuum cleaners must be bayesian},
	booktitle = {Proceedings of {AAA} {I} 1993 {Fall} {Symposium} {Series}: {Instantiating} {Real}-{World} {Agents}. {Raleigh}, {NC}},
	author = {Brittain, R. and D'Ambrosio, Bruce and Hall, Dearborn},
	year = {1993},
	pages = {19--22},
	file = {Brittain et al_1993_Autonomous vacuum cleaners must be bayesian.pdf:/home/phyl/Zotero/storage/KRUSGQTY/Brittain et al_1993_Autonomous vacuum cleaners must be bayesian.pdf:application/pdf},
}

@inproceedings{yamamoto_sozzy_1993,
	title = {Sozzy: {A} hormone-driven autonomous vacuum cleaner},
	volume = {2058},
	shorttitle = {Sozzy},
	booktitle = {{SPIE} {Proceedings}},
	author = {Yamamoto, Masaki},
	year = {1993},
	file = {Yamamoto_1993_Sozzy.pdf:/home/phyl/Zotero/storage/JFAF2LGC/Yamamoto_1993_Sozzy.pdf:application/pdf},
}

@inproceedings{coombs_robovac_1993,
	title = {{RoboVac} and the cat will get along famously},
	booktitle = {Proceedings of {AAA} {I} 1993 {Fall} {Symposium} {Series}: {Instantiating} {Real}-{World} {Agents}. {Raleigh}, {NC}},
	author = {Coombs, David},
	year = {1993},
	pages = {34--36},
	file = {Coombs_1993_RoboVac and the cat will get along famously.pdf:/home/phyl/Zotero/storage/YH7C5VND/Coombs_1993_RoboVac and the cat will get along famously.pdf:application/pdf},
}

@article{mori_bukimi_1970,
	title = {Bukimi no tani [the uncanny valley]},
	volume = {7},
	journal = {Energy},
	author = {Mori, Masahiro},
	year = {1970},
	pages = {33--35},
	file = {Snapshot:/home/phyl/Zotero/storage/GZJ947VQ/10027463083.html:text/html},
}

@article{andersen_robots_2010,
	title = {Robots {On} the {Move} from the {Production} {Line} to the {Service} {Sector}: {The} {Grand} {Challenges} for {Contractors}, {Workers}, and {Management}},
	abstract = {This paper presents a study on robot vacuum cleaning within the Danish public sector. Contrasting conventional images of robots as ineffective and technologically immature, we put forward the proposition that vacuum cleaning robots are at par with or better cleaning quality achieved by conventional vacuuming. Although the financial cost-benefit analysis provides inconclusive results, the case study reported here indicates that robots are mature enough to be adopted in the cleaning of the office environment. In the adoption of robots, we identify key challenges for management, contractors, and workers.},
	language = {en},
	author = {Andersen, Kim Normann and Medaglia, Rony and Gimpel, Gregory and Sjølin, Peter and Mikkelsen, Lene Skov},
	year = {2010},
	pages = {7},
	file = {Andersen et al. - 2010 - Robots On the Move from the Production Line to the.pdf:/home/phyl/Zotero/storage/S3TADFA5/Andersen et al. - 2010 - Robots On the Move from the Production Line to the.pdf:application/pdf},
}

@article{nitsch_emotions_2014,
	title = {Emotions in robot psychology},
	volume = {108},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/s00422-014-0594-6},
	doi = {10.1007/s00422-014-0594-6},
	abstract = {In his famous thought experiments on synthetic vehicles, Valentino Braitenberg stipulated that simple stimulus-response reactions in an organism could evoke the appearance of complex behavior, which, to the unsuspecting human observer, may even appear to be driven by emotions such as fear, aggression and even love (Braitenberg 1984). In fact, humans appear to have a strong propensity to anthropomorphize, driven by our inherent desire for predictability that will quickly lead us to discern patterns, cause-and-effect relationships and yes, emotions, in animated entities, be they natural or artificial. But might there be reasons, that we should intentionally “implement” emotions into artificial entities, such as robots? How would we proceed in creating robot emotions? And what, if any, are the ethical implications of creating “emotional” robots? The following article aims to shed some light on these questions with a multidisciplinary review of recent empirical investigations into the various facets of emotions in robot psychology.},
	language = {en},
	number = {5},
	urldate = {2021-11-21},
	journal = {Biological Cybernetics},
	author = {Nitsch, V. and Popp, M.},
	month = oct,
	year = {2014},
	pages = {621--629},
	file = {Nitsch and Popp - 2014 - Emotions in robot psychology.pdf:/home/phyl/Zotero/storage/N5Z5G65Z/Nitsch and Popp - 2014 - Emotions in robot psychology.pdf:application/pdf;Emotions in robot psychology:/home/phyl/Zotero/storage/ZFDRY2HQ/s00422-014-0594-6.pdf.pdf:application/pdf},
}

@inproceedings{lutkebohle_bielefeld_2010,
	address = {Anchorage, AK},
	title = {The bielefeld anthropomorphic robot head \&\#{x201C};{Flobi}\&\#{x201D};},
	isbn = {978-1-4244-5038-1},
	url = {http://ieeexplore.ieee.org/document/5509173/},
	doi = {10.1109/ROBOT.2010.5509173},
	abstract = {A robot’s head is important both for directional sensors and, in human-directed robotics, as the single most visible interaction interface. However, designing a robot’s head faces contradicting requirements when integrating powerful sensing with social expression. Furher, reactions of the general public show that current head designs often cause negative user reactions and distract from the functional capabilities.},
	language = {en},
	urldate = {2021-11-21},
	booktitle = {2010 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Lütkebohle, Ingo and Hegel, Frank and Schulz, Simon and Hackel, Matthias and Wrede, Britta and Wachsmuth, Sven and Sagerer, Gerhard},
	month = may,
	year = {2010},
	pages = {3384--3391},
	file = {Lütkebohle et al. - 2010 - The bielefeld anthropomorphic robot head &#x201C\;F.pdf:/home/phyl/Zotero/storage/2SLEN54I/Lütkebohle et al. - 2010 - The bielefeld anthropomorphic robot head &#x201C\;F.pdf:application/pdf;The bielefeld anthropomorphic robot head &#x201C\;Flobi&#x201D\;:/home/phyl/Zotero/storage/88HFEQTH/robot.2010.5509173.pdf.pdf:application/pdf},
}

@inproceedings{delaunay_towards_2009,
	address = {Toyama, Japan},
	title = {Towards retro-projected robot faces: {An} alternative to mechatronic and android faces},
	isbn = {978-1-4244-5081-7},
	shorttitle = {Towards retro-projected robot faces},
	url = {http://ieeexplore.ieee.org/document/5326314/},
	doi = {10.1109/ROMAN.2009.5326314},
	abstract = {This paper presents a new implementation of a robot face using retro-projection of a video stream onto a semitransparent facial mask. The technology is contrasted against mechatronic robot faces, of which Kismet is a typical example, and android robot faces, as used on the Ishiguro robots. The paper highlights the strengths of Retro-projected Animated Faces (RAF) technology (with cost, ﬂexibility and robustness being notably strong) and discusses potential developments.},
	language = {en},
	urldate = {2021-11-21},
	booktitle = {{RO}-{MAN} 2009 - {The} 18th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}},
	publisher = {IEEE},
	author = {Delaunay, Frederic and de Greeff, Joachim and Belpaeme, Tony},
	month = sep,
	year = {2009},
	pages = {306--311},
	file = {Delaunay et al. - 2009 - Towards retro-projected robot faces An alternativ.pdf:/home/phyl/Zotero/storage/XHL9VGTP/Delaunay et al. - 2009 - Towards retro-projected robot faces An alternativ.pdf:application/pdf;Towards retro-projected robot faces\: An alternative to mechatronic and android faces:/home/phyl/Zotero/storage/9HI45DE9/roman.2009.5326314.pdf.pdf:application/pdf},
}

@article{chaminade_brain_2010,
	title = {Brain {Response} to a {Humanoid} {Robot} in {Areas} {Implicated} in the {Perception} of {Human} {Emotional} {Gestures}},
	volume = {5},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0011577},
	doi = {10.1371/journal.pone.0011577},
	abstract = {Background The humanoid robot WE4-RII was designed to express human emotions in order to improve human-robot interaction. We can read the emotions depicted in its gestures, yet might utilize different neural processes than those used for reading the emotions in human agents. Methodology Here, fMRI was used to assess how brain areas activated by the perception of human basic emotions (facial expression of Anger, Joy, Disgust) and silent speech respond to a humanoid robot impersonating the same emotions, while participants were instructed to attend either to the emotion or to the motion depicted. Principal Findings Increased responses to robot compared to human stimuli in the occipital and posterior temporal cortices suggest additional visual processing when perceiving a mechanical anthropomorphic agent. In contrast, activity in cortical areas endowed with mirror properties, like left Broca's area for the perception of speech, and in the processing of emotions like the left anterior insula for the perception of disgust and the orbitofrontal cortex for the perception of anger, is reduced for robot stimuli, suggesting lesser resonance with the mechanical agent. Finally, instructions to explicitly attend to the emotion significantly increased response to robot, but not human facial expressions in the anterior part of the left inferior frontal gyrus, a neural marker of motor resonance. Conclusions Motor resonance towards a humanoid robot, but not a human, display of facial emotion is increased when attention is directed towards judging emotions. Significance Artificial agents can be used to assess how factors like anthropomorphism affect neural response to the perception of human actions.},
	language = {en},
	number = {7},
	urldate = {2021-11-21},
	journal = {PLOS ONE},
	author = {Chaminade, Thierry and Zecca, Massimiliano and Blakemore, Sarah-Jayne and Takanishi, Atsuo and Frith, Chris D. and Micera, Silvestro and Dario, Paolo and Rizzolatti, Giacomo and Gallese, Vittorio and Umiltà, Maria Alessandra},
	month = jul,
	year = {2010},
	note = {Publisher: Public Library of Science},
	keywords = {Emotions, Face, Facial expressions, Functional magnetic resonance imaging, Robots, Sensory perception, Speech signal processing, Vision},
	pages = {e11577},
	file = {Chaminade et al_2010_Brain Response to a Humanoid Robot in Areas Implicated in the Perception of.pdf:/home/phyl/Zotero/storage/MUT6KFS3/Chaminade et al_2010_Brain Response to a Humanoid Robot in Areas Implicated in the Perception of.pdf:application/pdf;Full Text PDF:/home/phyl/Zotero/storage/27HDU8JS/Chaminade et al. - 2010 - Brain Response to a Humanoid Robot in Areas Implic.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/HTQZUCYG/article.html:text/html},
}

@article{kaminski_robots_2015,
	title = {Robots in the {Home}: {What} {Will} {We} {Have} {Agreed} {To}?},
	issn = {1556-5068},
	shorttitle = {Robots in the {Home}},
	url = {http://www.ssrn.com/abstract=2592500},
	doi = {10.2139/ssrn.2592500},
	language = {en},
	urldate = {2021-11-21},
	journal = {SSRN Electronic Journal},
	author = {Kaminski, Margot E.},
	year = {2015},
	file = {Kaminski - 2015 - Robots in the Home What Will We Have Agreed To.pdf:/home/phyl/Zotero/storage/MQSKSBD5/Kaminski - 2015 - Robots in the Home What Will We Have Agreed To.pdf:application/pdf;Robots in the Home\: What Will We Have Agreed To?:/home/phyl/Zotero/storage/ZT2NQGG8/ssrn.2592500.pdf.pdf:application/pdf},
}

@article{ihamaki_robot_2021,
	title = {Robot {Pets} as “{Serious} {Toys}”- {Activating} {Social} and {Emotional} {Experiences} of {Elderly} {People}},
	issn = {1572-9419},
	url = {https://doi.org/10.1007/s10796-021-10175-z},
	doi = {10.1007/s10796-021-10175-z},
	abstract = {When robots are used as part of meaningful play, for example to enhance wellbeing, they can be considered “serious toys”. Our study examines the potential of robotic pet toys viewed as companions, which activate social and emotional experiences of the elderly by increasing their wellbeing. In order to study the benefits of using Golden Pup, a commercial robot dog, we designed and performed a research intervention at a senior day activity center with 10 participants of ages 65–80+ years who were joined by a playful group of preschoolers. In this study, we were mainly interested in the firsthand user experiences.This study suggests how robotic pets can be used to activate the social and emotional experiences of elderly, and illustrated the role of building a relationship with a robotic pet. We present novel results on how a robot dog with a natural interface (NUI) may be used to evoke social and emotional experiences in older adults as part of playful, intergenerational group activities.},
	language = {en},
	urldate = {2021-11-21},
	journal = {Information Systems Frontiers},
	author = {Ihamäki, Pirita and Heljakka, Katriina},
	month = aug,
	year = {2021},
	file = {Ihamäki_Heljakka_2021_Robot Pets as “Serious Toys”- Activating Social and Emotional Experiences of.pdf:/home/phyl/Zotero/storage/H9HCDJEJ/Ihamäki_Heljakka_2021_Robot Pets as “Serious Toys”- Activating Social and Emotional Experiences of.pdf:application/pdf;Springer Full Text PDF:/home/phyl/Zotero/storage/MWNIP759/Ihamäki and Heljakka - 2021 - Robot Pets as “Serious Toys”- Activating Social an.pdf:application/pdf},
}

@article{denning_spotlight_nodate,
	title = {A {Spotlight} on {Security} and {Privacy} {Risks} with {Future} {Household} {Robots}: {Attacks} and {Lessons}},
	abstract = {Future homes will be populated with large numbers of robots with diverse functionalities, ranging from chore robots to elder care robots to entertainment robots. While household robots will offer numerous beneﬁts, they also have the potential to introduce new security and privacy vulnerabilities into the home. Our research consists of three parts. First, to serve as a foundation for our study, we experimentally analyze three of today’s household robots for security and privacy vulnerabilities: the WowWee Rovio, the Erector Spykee, and the WowWee RoboSapien V2. Second, we synthesize the results of our experimental analyses and identify key lessons and challenges for securing future household robots. Finally, we use our experiments and lessons learned to construct a set of design questions aimed at facilitating the future development of household robots that are secure and preserve their users’ privacy.},
	language = {en},
	author = {Denning, Tamara and Matuszek, Cynthia and Koscher, Karl and Smith, Joshua R and Kohno, Tadayoshi},
	pages = {10},
	file = {Denning et al. - A Spotlight on Security and Privacy Risks with Fut.pdf:/home/phyl/Zotero/storage/X28BNXFI/Denning et al. - A Spotlight on Security and Privacy Risks with Fut.pdf:application/pdf},
}

@article{hasan_human_2021,
	title = {Human face detection techniques: {A} comprehensive review and future research directions},
	volume = {10},
	shorttitle = {Human face detection techniques},
	number = {19},
	journal = {Electronics},
	author = {Hasan, Md Khaled and Ahsan, Md and Newaz, S. H. and Lee, Gyu Myoung},
	year = {2021},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {2354},
	file = {Hasan et al_2021_Human face detection techniques.pdf:/home/phyl/Zotero/storage/8F6PZMAK/Hasan et al_2021_Human face detection techniques.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/8MBR9XXN/2354.html:text/html},
}

@article{du_elements_2021,
	title = {The {Elements} of {End}-to-end {Deep} {Face} {Recognition}: {A} {Survey} of {Recent} {Advances}},
	shorttitle = {The {Elements} of {End}-to-end {Deep} {Face} {Recognition}},
	url = {http://arxiv.org/abs/2009.13290},
	abstract = {Face recognition is one of the most popular and long-standing topics in computer vision. With the recent development of deep learning techniques and large-scale datasets, deep face recognition has made remarkable progress and been widely used in many real-world applications. Given a natural image or video frame as input, an end-to-end deep face recognition system outputs the face feature for recognition. To achieve this, a typical end-to-end system is generally built with three key elements: face detection, face alignment, and face representation. The face detection locates faces in the image or frame. Then, the face alignment is proceeded to calibrate the faces to a canonical view and crop them to a normalized pixel size. Finally, in the stage of face representation, the discriminative features are extracted from the aligned face for recognition. Nowadays, all of the three elements are fulfilled by the technique of deep convolutional neural network. In this survey article, we present a comprehensive review about the recent advance of each element of the end-to-end deep face recognition, since the thriving deep learning techniques have greatly improved the capability of them. To start with, we present an overview of the end-to-end deep face recognition. Then, we review the advance of each element, respectively, covering many aspects such as the to-date algorithm designs, evaluation metrics, datasets, performance comparison, existing challenges, and promising directions for future research. Also, we provide a detailed discussion about the effect of each element on its subsequent elements and the holistic system. Through this survey, we wish to bring contributions in two aspects: first, readers can conveniently identify the methods which are quite strong-baseline style in the subcategory for further exploration; second, one can also employ suitable methods for establishing a state-of-the-art end-to-end face recognition system from scratch.},
	language = {en},
	urldate = {2021-11-24},
	journal = {arXiv:2009.13290 [cs]},
	author = {Du, Hang and Shi, Hailin and Zeng, Dan and Zhang, Xiaoping and Mei, Tao},
	month = sep,
	year = {2021},
	note = {arXiv: 2009.13290},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Du et al. - 2021 - The Elements of End-to-end Deep Face Recognition .pdf:/home/phyl/Zotero/storage/7DHSPDL5/Du et al. - 2021 - The Elements of End-to-end Deep Face Recognition .pdf:application/pdf},
}

@article{du_elements_2021-1,
	title = {The {Elements} of {End}-to-end {Deep} {Face} {Recognition}: {A} {Survey} of {Recent} {Advances}},
	shorttitle = {The {Elements} of {End}-to-end {Deep} {Face} {Recognition}},
	url = {http://arxiv.org/abs/2009.13290},
	abstract = {Face recognition is one of the most popular and long-standing topics in computer vision. With the recent development of deep learning techniques and large-scale datasets, deep face recognition has made remarkable progress and been widely used in many real-world applications. Given a natural image or video frame as input, an end-to-end deep face recognition system outputs the face feature for recognition. To achieve this, a typical end-to-end system is generally built with three key elements: face detection, face alignment, and face representation. The face detection locates faces in the image or frame. Then, the face alignment is proceeded to calibrate the faces to a canonical view and crop them to a normalized pixel size. Finally, in the stage of face representation, the discriminative features are extracted from the aligned face for recognition. Nowadays, all of the three elements are fulfilled by the technique of deep convolutional neural network. In this survey article, we present a comprehensive review about the recent advance of each element of the end-to-end deep face recognition, since the thriving deep learning techniques have greatly improved the capability of them. To start with, we present an overview of the end-to-end deep face recognition. Then, we review the advance of each element, respectively, covering many aspects such as the to-date algorithm designs, evaluation metrics, datasets, performance comparison, existing challenges, and promising directions for future research. Also, we provide a detailed discussion about the effect of each element on its subsequent elements and the holistic system. Through this survey, we wish to bring contributions in two aspects: first, readers can conveniently identify the methods which are quite strong-baseline style in the subcategory for further exploration; second, one can also employ suitable methods for establishing a state-of-the-art end-to-end face recognition system from scratch.},
	language = {en},
	urldate = {2021-11-24},
	journal = {arXiv:2009.13290 [cs]},
	author = {Du, Hang and Shi, Hailin and Zeng, Dan and Zhang, Xiaoping and Mei, Tao},
	month = sep,
	year = {2021},
	note = {arXiv: 2009.13290},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Du et al. - 2021 - The Elements of End-to-end Deep Face Recognition .pdf:/home/phyl/Zotero/storage/2FW4RH9U/Du et al. - 2021 - The Elements of End-to-end Deep Face Recognition .pdf:application/pdf},
}

@inproceedings{r_survey_2021,
	title = {A {Survey} on {Recent} {Techniques} in {Face} {Recognition} for {Various} {Databases}},
	doi = {10.1109/CSNT51715.2021.9509641},
	abstract = {Face recognition is a significant biometric application in Image Processing. In the research zone, the Face recognition topic persistently continued due to the most significant challenge such that illumination effects. This may occur due to camera type, focus, resolutions, illumination directions, Face pose conditions, aging, expression type, and so forth. To improve the performance of Face Recognition System [FRS] different face detection, Illumination Pre-processing, feature extraction, classification techniques are needed to be utilized in a hybrid manner. This paper reviews various face recognition methods, and also their effective performance on different combinations.},
	booktitle = {2021 10th {IEEE} {International} {Conference} on {Communication} {Systems} and {Network} {Technologies} ({CSNT})},
	author = {R, Vijaya Kumar H and M., Mathivanan},
	month = jun,
	year = {2021},
	note = {ISSN: 2329-7182},
	keywords = {Cameras, Classifier, Databases, Face detection, Face recognition, Feature extraction, Lighting, Pre-processing, Real-time systems, Support vector machines},
	pages = {345--349},
	file = {IEEE Xplore Abstract Record:/home/phyl/Zotero/storage/64MTK2YD/9509641.html:text/html},
}

@article{liu_survey_2021,
	title = {A survey and performance evaluation of deep learning methods for small object detection},
	volume = {172},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000439},
	doi = {10.1016/j.eswa.2021.114602},
	abstract = {In computer vision, significant advances have been made on object detection with the rapid development of deep convolutional neural networks (CNN). This paper provides a comprehensive review of recently developed deep learning methods for small object detection. We summarize challenges and solutions of small object detection, and present major deep learning techniques, including fusing feature maps, adding context information, balancing foreground-background examples, and creating sufficient positive examples. We discuss related techniques developed in four research areas, including generic object detection, face detection, object detection in aerial imagery, and segmentation. In addition, this paper compares the performances of several leading deep learning methods for small object detection, including YOLOv3, Faster R-CNN, and SSD, based on three large benchmark datasets of small objects. Our experimental results show that while the detection accuracy on small objects by these deep learning methods was low, less than 0.4, Faster R-CNN performed the best, while YOLOv3 was a close second.},
	language = {en},
	urldate = {2021-11-24},
	journal = {Expert Systems with Applications},
	author = {Liu, Yang and Sun, Peng and Wergeles, Nickolas and Shang, Yi},
	month = jun,
	year = {2021},
	keywords = {Computer vision, Convolutional neural networks, Deep learning, Small object detection},
	pages = {114602},
	file = {Liu et al_2021_A survey and performance evaluation of deep learning methods for small object.pdf:/home/phyl/Zotero/storage/5UJR94P5/Liu et al_2021_A survey and performance evaluation of deep learning methods for small object.pdf:application/pdf},
}

@article{zhou_survey_2018,
	title = {Survey of {Face} {Detection} on {Low}-quality {Images}},
	url = {http://arxiv.org/abs/1804.07362},
	abstract = {Face detection is a well-explored problem. Many challenges on face detectors like extreme pose, illumination, low resolution and small scales are studied in the previous work. However, previous proposed models are mostly trained and tested on good-quality images which are not always the case for practical applications like surveillance systems. In this paper, we ﬁrst review the current state-of-the-art face detectors and their performance on benchmark dataset FDDB, and compare the design protocols of the algorithms. Secondly, we investigate their performance degradation while testing on low-quality images with different levels of blur, noise, and contrast. Our results demonstrate that both hand-crafted and deep-learning based face detectors are not robust enough for low-quality images. It inspires researchers to produce more robust design for face detection in the wild.},
	language = {en},
	urldate = {2021-11-25},
	journal = {arXiv:1804.07362 [cs]},
	author = {Zhou, Yuqian and Liu, Ding and Huang, Thomas},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.07362},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhou et al. - 2018 - Survey of Face Detection on Low-quality Images.pdf:/home/phyl/Zotero/storage/SK2FAIG9/Zhou et al. - 2018 - Survey of Face Detection on Low-quality Images.pdf:application/pdf},
}

@article{minaee_going_2021,
	title = {Going {Deeper} {Into} {Face} {Detection}: {A} {Survey}},
	shorttitle = {Going {Deeper} {Into} {Face} {Detection}},
	url = {http://arxiv.org/abs/2103.14983},
	abstract = {Face detection is a crucial ﬁrst step in many facial recognition and face analysis systems. Early approaches for face detection were mainly based on classiﬁers built on top of hand-crafted features extracted from local image regions, such as Haar Cascades and Histogram of Oriented Gradients. However, these approaches were not powerful enough to achieve a high accuracy on images of from uncontrolled environments. With the breakthrough work in image classiﬁcation using deep neural networks in 2012, there has been a huge paradigm shift in face detection. Inspired by the rapid progress of deep learning in computer vision, many deep learning based frameworks have been proposed for face detection over the past few years, achieving signiﬁcant improvements in accuracy. In this work, we provide a detailed overview of some of the most representative deep learning based face detection methods by grouping them into a few major categories, and present their core architectural designs and accuracies on popular benchmarks. We also describe some of the most popular face detection datasets. Finally, we discuss some current challenges in the ﬁeld, and suggest potential future research directions.},
	language = {en},
	urldate = {2021-11-25},
	journal = {arXiv:2103.14983 [cs]},
	author = {Minaee, Shervin and Luo, Ping and Lin, Zhe and Bowyer, Kevin},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.14983},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Minaee et al. - 2021 - Going Deeper Into Face Detection A Survey.pdf:/home/phyl/Zotero/storage/GX3LHCY3/Minaee et al. - 2021 - Going Deeper Into Face Detection A Survey.pdf:application/pdf},
}

@article{kumar_face_2019,
	title = {Face detection techniques: a review},
	volume = {52},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Face detection techniques},
	url = {http://link.springer.com/10.1007/s10462-018-9650-2},
	doi = {10.1007/s10462-018-9650-2},
	abstract = {With the marvelous increase in video and image database there is an incredible need of automatic understanding and examination of information by the intelligent systems as manually it is getting to be plainly distant. Face plays a major role in social intercourse for conveying identity and feelings of a person. Human beings have not tremendous ability to identify different faces than machines. So, automatic face detection system plays an important role in face recognition, facial expression recognition, head-pose estimation, human–computer interaction etc. Face detection is a computer technology that determines the location and size of a human face in a digital image. Face detection has been a standout amongst topics in the computer vision literature. This paper presents a comprehensive survey of various techniques explored for face detection in digital images. Different challenges and applications of face detection are also presented in this paper. At the end, different standard databases for face detection are also given with their features. Furthermore, we organize special discussions on the practical aspects towards the development of a robust face detection system and conclude this paper with several promising directions for future research.},
	language = {en},
	number = {2},
	urldate = {2021-11-28},
	journal = {Artificial Intelligence Review},
	author = {Kumar, Ashu and Kaur, Amandeep and Kumar, Munish},
	month = aug,
	year = {2019},
	pages = {927--948},
	file = {Kumar et al. - 2019 - Face detection techniques a review.pdf:/home/phyl/Zotero/storage/9WDEUI7Y/Kumar et al. - 2019 - Face detection techniques a review.pdf:application/pdf;Face detection techniques\: a review:/home/phyl/Zotero/storage/V2NYWA3J/s10462-018-9650-2.pdf.pdf:application/pdf},
}

@inproceedings{ye_face_2021,
	title = {Face {SSD}: {A} {Real}-time {Face} {Detector} based on {SSD}},
	shorttitle = {Face {SSD}},
	doi = {10.23919/CCC52363.2021.9550294},
	abstract = {Face detection has made substantial progress in recent years. In many applications, face detectors must run on mobile devices or embedded devices. Due to the limited computing resource in such scenarios, it is usually difficult to meet the need of both accuracy and speed. Most detectors cannot detect small face accurately or the detection speed will slow down. To address this challenge, we propose a novel face detector in which the basic framework is a single shot multibox detector (SSD), and name it Face SSD. In order to accelerate the detection speed of Face SSD and improve the detection accuracy, we make contributions in the following three aspects: we improved the structure of ShuffleNet V2 and use it as the backbone network; then we proposed a modified prediction module (MPM) to improve recall of small faces; finally we introduced a scale-equitable face detection framework to match face better. Experimental results show that Face SSD runs 40 frames per second (FPS) on CPU and 110 FPS on GPU. We train Face SSD on WIDER FACE - a face detection benchmark. The detector has also achieved excellent performance on FDDB, a benchmark for face detection in unconstrained settings.},
	booktitle = {2021 40th {Chinese} {Control} {Conference} ({CCC})},
	author = {Ye, Bin and Shi, Yunlin and Li, Huijun and Li, Liuchuan and Tong, Shuo},
	month = jul,
	year = {2021},
	note = {ISSN: 1934-1768},
	keywords = {Real-time systems, Benchmark testing, Computational modeling, Convolution, Detectors, Face Detector, Graphics processing units, MPM, Performance evaluation, Real-time, ShuffleNet V2, SSD},
	pages = {8445--8450},
	file = {IEEE Xplore Abstract Record:/home/phyl/Zotero/storage/UI3N9K8X/9550294.html:text/html},
}

@article{mccready_real-time_nodate,
	title = {Real-{Time} {Face} {Detection} on a {Configurable} {Hardware} {Platform}},
	abstract = {Automated object detection is desirable because locating important structures in an image is a fundamental operation in machine vision. The biggest obstacle to realizing this goal is the inherent computational complexity of the problem. The focus of this research is to develop an object detection system that operates on real-time video data in hardware. Using human faces as the target object, we develop a detection method that is both accurate and efficient, and then implement it on a large programmable hardware system. Using a programmable system reduced the time and cost required to create a working hardware prototype. The implementation runs at 30 frames per second, which is approximately 1000 times faster than the same algorithm running in software and approximately 90 to 6000 times faster than the reported speed of other software algorithms.},
	language = {en},
	author = {McCready, Rob},
	pages = {83},
	file = {McCready - Real-Time Face Detection on a Configurable Hardwar.pdf:/home/phyl/Zotero/storage/RU6BPT9W/McCready - Real-Time Face Detection on a Configurable Hardwar.pdf:application/pdf},
}

@article{dubois_master_nodate,
	title = {Master thesis :  {Facial} recognition using deep neural networks.},
	language = {en},
	author = {Dubois, A},
	pages = {130},
	file = {Dubois - Master thesis   Facial recognition using deep neu.pdf:/home/phyl/Zotero/storage/PYR55ZPI/Dubois - Master thesis   Facial recognition using deep neu.pdf:application/pdf},
}

@article{bruce_understanding_1986,
	title = {Understanding face recognition},
	volume = {77},
	number = {3},
	journal = {British journal of psychology},
	author = {Bruce, Vicki and Young, Andy},
	year = {1986},
	note = {Publisher: Wiley Online Library},
	pages = {305--327},
	file = {Bruce_Young_1986_Understanding face recognition.pdf:/home/phyl/Zotero/storage/X9TJUQAB/Bruce_Young_1986_Understanding face recognition.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/3T86KTXV/j.2044-8295.1986.tb02199.html:text/html},
}

@article{yang_detecting_2002,
	title = {Detecting faces in images: {A} survey},
	volume = {24},
	shorttitle = {Detecting faces in images},
	number = {1},
	journal = {IEEE Transactions on pattern analysis and machine intelligence},
	author = {Yang, Ming-Hsuan and Kriegman, David J. and Ahuja, Narendra},
	year = {2002},
	note = {Publisher: IEEE},
	pages = {34--58},
	file = {Yang et al_2002_Detecting faces in images.pdf:/home/phyl/Zotero/storage/TWGSABTQ/Yang et al_2002_Detecting faces in images.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/XBRKWGUA/982883.html:text/html},
}

@inproceedings{zhu_face_2012,
	title = {Face detection, pose estimation, and landmark localization in the wild},
	booktitle = {2012 {IEEE} conference on computer vision and pattern recognition},
	publisher = {IEEE},
	author = {Zhu, Xiangxin and Ramanan, Deva},
	year = {2012},
	pages = {2879--2886},
	file = {Zhu_Ramanan_2012_Face detection, pose estimation, and landmark localization in the wild.pdf:/home/phyl/Zotero/storage/44L6IYNC/Zhu_Ramanan_2012_Face detection, pose estimation, and landmark localization in the wild.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/3VNZTELL/6248014.html:text/html},
}

@techreport{jain_fddb_2010,
	title = {Fddb: {A} benchmark for face detection in unconstrained settings},
	shorttitle = {Fddb},
	institution = {UMass Amherst technical report},
	author = {Jain, Vidit and Learned-Miller, Erik},
	year = {2010},
	file = {Jain_Learned-Miller_2010_Fddb.pdf:/home/phyl/Zotero/storage/MMYDZ5DH/Jain_Learned-Miller_2010_Fddb.pdf:application/pdf},
}

@article{yan_face_2014,
	title = {Face detection by structural models},
	volume = {32},
	doi = {10.1016/j.imavis.2013.12.004},
	number = {10},
	journal = {Image and Vision Computing},
	author = {Yan, Junjie and Zhang, Xuzong and Lei, Zhen and Li, Stan Z.},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {790--799},
	file = {Yan et al_2014_Face detection by structural models.pdf:/home/phyl/Zotero/storage/B97TR7HN/Yan et al_2014_Face detection by structural models.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/J3F5CTIQ/S0262885613001765.html:text/html;Face detection by structural models:/home/phyl/Zotero/storage/JRG7WAA2/j.imavis.2013.12.004.pdf.pdf:application/pdf},
}

@inproceedings{berg_names_2004,
	title = {Names and faces in the news},
	volume = {2},
	booktitle = {Proceedings of the 2004 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, 2004. {CVPR} 2004.},
	publisher = {IEEE},
	author = {Berg, Tamara L. and Berg, Alexander C. and Edwards, Jaety and Maire, Michael and White, Ryan and Teh, Yee-Whye and Learned-Miller, Erik and Forsyth, David A.},
	year = {2004},
	pages = {II--II},
	file = {Berg et al_2004_Names and faces in the news.pdf:/home/phyl/Zotero/storage/FPTFJ85E/Berg et al_2004_Names and faces in the news.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/Y9QTUEDW/1315253.html:text/html},
}

@article{everingham_pascal_2010,
	title = {The pascal visual object classes (voc) challenge},
	volume = {88},
	doi = {10.1007/s11263-009-0275-4},
	number = {2},
	journal = {International journal of computer vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
	year = {2010},
	note = {Publisher: Springer},
	pages = {303--338},
	file = {Snapshot:/home/phyl/Zotero/storage/YTU26N68/s11263-009-0275-4.html:text/html;The pascal visual object classes (voc) challenge:/home/phyl/Zotero/storage/KLMLZL7X/s11263-009-0275-4.pdf.pdf:application/pdf},
}

@misc{noauthor_wider_nodate,
	title = {{WIDER} {FACE}: {A} {Face} {Detection} {Benchmark}},
	url = {http://shuoyang1213.me/WIDERFACE/},
	urldate = {2021-12-20},
	file = {WIDER FACE\: A Face Detection Benchmark:/home/phyl/Zotero/storage/2WZ2TCVP/WIDERFACE.html:text/html},
}

@misc{noauthor_fine-grained_nodate,
	title = {Fine-grained {Evaluation} on {Face} {Detection} in the {Wild}},
	url = {http://www.cbsr.ia.ac.cn/faceevaluation/},
	urldate = {2021-12-20},
	file = {Fine-grained Evaluation on Face Detection in the Wild:/home/phyl/Zotero/storage/6MN2Q56W/faceevaluation.html:text/html},
}

@misc{patriciaflanagannistgov_ijb-dataset_2015,
	type = {text},
	title = {{IJB}-{A} {Dataset} {Request} {Form}},
	url = {https://www.nist.gov/itl/iad/image-group/ijb-dataset-request-form},
	abstract = {By downloading the IARPA Janus Benchmark A (IJB-A) dataset, the Receiving Entity agrees to:},
	language = {en},
	urldate = {2021-12-20},
	journal = {NIST},
	author = {patricia.flanagan@nist.gov},
	month = sep,
	year = {2015},
	note = {Last Modified: 2019-03-29T14:31-04:00},
	file = {Snapshot:/home/phyl/Zotero/storage/MEGYXKG3/ijb-dataset-request-form.html:text/html},
}

@misc{noauthor_fddb_nodate,
	title = {{FDDB} : {Main}},
	url = {http://vis-www.cs.umass.edu/fddb/index.html},
	urldate = {2021-12-20},
	file = {FDDB \: Main:/home/phyl/Zotero/storage/ZME4KSH4/index.html:text/html},
}

@misc{noauthor_pascal_nodate,
	title = {The {PASCAL} {Object} {Recognition} {Database} {Collection}},
	url = {http://host.robots.ox.ac.uk/pascal/VOC/databases.html},
	urldate = {2021-12-20},
	file = {The PASCAL Object Recognition Database Collection:/home/phyl/Zotero/storage/2T3488KE/databases.html:text/html},
}

@inproceedings{bin_yang_fine-grained_2015,
	address = {Ljubljana},
	title = {Fine-grained evaluation on face detection in the wild},
	isbn = {978-1-4799-6026-2},
	url = {http://ieeexplore.ieee.org/document/7163158/},
	doi = {10.1109/FG.2015.7163158},
	abstract = {Current evaluation datasets for face detection, which is of great value in real-world applications, are still somewhat out-of-date. We propose a new face detection dataset MALF (short for Multi-Attribute Labelled Faces), which contains 5,250 images collected from the Internet and ∼12,000 labelled faces. The MALF dataset highlights in two main features: 1) It is the largest dataset for evaluation of face detection in the wild, and the annotation of multiple facial attributes makes it possible for ﬁne-grained performance analysis. 2) To reveal the ‘true’ performances of algorithms in practice, MALF adopts an evaluation metric that puts stress on the recall rate at a relatively low false alarm rate. Besides providing a large dataset for face detection evaluation, this paper also collects more than 20 state-of-the-art algorithms, both from academia and industry, and conducts a ﬁne-grained comparative evaluation of these algorithms, which can be considered as a summary of past advances made in face detection. The dataset and up-to-date results of the evaluation can be found at http: //www.cbsr.ia.ac.cn/faceevaluation/.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {2015 11th {IEEE} {International} {Conference} and {Workshops} on {Automatic} {Face} and {Gesture} {Recognition} ({FG})},
	publisher = {IEEE},
	author = {{Bin Yang} and {Junjie Yan} and {Zhen Lei} and Li, Stan Z.},
	month = may,
	year = {2015},
	pages = {1--7},
	file = {Bin Yang et al. - 2015 - Fine-grained evaluation on face detection in the w.pdf:/home/phyl/Zotero/storage/4TYVWQPE/Bin Yang et al. - 2015 - Fine-grained evaluation on face detection in the w.pdf:application/pdf;Fine-grained evaluation on face detection in the wild:/home/phyl/Zotero/storage/5TC9YCG4/FG.2015.7163158.pdf.pdf:application/pdf},
}

@article{naphade_large-scale_2006,
	title = {Large-scale concept ontology for multimedia},
	volume = {13},
	number = {3},
	journal = {IEEE multimedia},
	author = {Naphade, Milind and Smith, John R. and Tesic, Jelena and Chang, Shih-Fu and Hsu, Winston and Kennedy, Lyndon and Hauptmann, Alexander and Curtis, Jon},
	year = {2006},
	note = {Publisher: IEEE},
	pages = {86--91},
	file = {Naphade et al_2006_Large-scale concept ontology for multimedia.pdf:/home/phyl/Zotero/storage/Q37HM9R2/Naphade et al_2006_Large-scale concept ontology for multimedia.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/PXHAYUN3/1667983.html:text/html},
}

@inproceedings{zitnick_edge_2014,
	title = {Edge boxes: {Locating} object proposals from edges},
	shorttitle = {Edge boxes},
	booktitle = {European conference on computer vision},
	publisher = {Springer},
	author = {Zitnick, C. Lawrence and Dollár, Piotr},
	year = {2014},
	pages = {391--405},
	file = {Zitnick_Dollár_2014_Edge boxes.pdf:/home/phyl/Zotero/storage/RTDNSW9M/Zitnick_Dollár_2014_Edge boxes.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/TE2U7IRL/978-3-319-10602-1_26.html:text/html},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {The} latest in {Machine} {Learning}},
	url = {https://paperswithcode.com/},
	abstract = {Papers With Code highlights trending Machine Learning research and the code to implement it.},
	language = {en},
	urldate = {2021-12-20},
	file = {Snapshot:/home/phyl/Zotero/storage/SLEQA65P/paperswithcode.com.html:text/html},
}

@article{chan_man-machine_1965,
	title = {A man-machine facial recognition system: some preliminary results},
	shorttitle = {A man-machine facial recognition system},
	journal = {Panoramic Research Inc., Palo Alto, CA, USA1965},
	author = {Chan, H. and Bledsoe, W. W.},
	year = {1965},
}

@book{sakai_computer_1972,
	title = {Computer analysis and classification of photographs of human faces},
	publisher = {Kyoto University},
	author = {Sakai, Toshiyuki and Nagao, Makoto and Kanade, Takeo},
	year = {1972},
	file = {Sakai et al_1972_Computer analysis and classification of photographs of human faces.pdf:/home/phyl/Zotero/storage/DXYVB55Q/Sakai et al_1972_Computer analysis and classification of photographs of human faces.pdf:application/pdf},
}

@inproceedings{crow_summed-area_1984,
	title = {Summed-area tables for texture mapping},
	booktitle = {Proceedings of the 11th annual conference on {Computer} graphics and interactive techniques},
	author = {Crow, Franklin C.},
	year = {1984},
	pages = {207--212},
	file = {Crow_1984_Summed-area tables for texture mapping.pdf:/home/phyl/Zotero/storage/677LIKNG/Crow_1984_Summed-area tables for texture mapping.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/SJN8NANR/800031.html:text/html},
}

@article{meir_introduction_2003,
	title = {An introduction to boosting and leveraging},
	volume = {2600},
	issn = {0302-9743},
	doi = {10.1007/3-540-36434-x_4},
	abstract = {We provide an introduction to theoretical and practical aspects of Boosting and Ensemble learning, providing a useful reference for researchers in the field of Boosting as well as for those seeking to enter this fascinating area of research. We begin with a short background concerning the necessary learning theoretical foundations of weak learners ahd their linear combinations. We then point out the useful connection between Boosting and the Theory of Optimization, which facilitates the understanding of Boosting and later on enables us to move on to new Boosting algorithms, applicable to a broad spectrum of problems. In order to increase the relevance of the paper to practitioners, we have added remarks, pseudo code, "tricks of the trade", and algorithmic considerations where appropriate. Finally, we illustrate the usefulness of Boosting algorithms by giving an overview of some existing applications. The main ideas are illustrated on the problem of binary classification, although several extensions are discussed. © Springer-Verlag Berlin Heidelberg 2003.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Meir, R. and Rätsch, G.},
	year = {2003},
	note = {ISBN: 9783540005292},
	pages = {118--183},
	annote = {Cited By :241},
	file = {Meir_Rätsch_2003_An introduction to boosting and leveraging.pdf:/home/phyl/Zotero/storage/VGMD47WM/Meir_Rätsch_2003_An introduction to boosting and leveraging.pdf:application/pdf},
}

@article{freund_decision-theoretic_1997,
	title = {A {Decision}-{Theoretic} {Generalization} of {On}-{Line} {Learning} and an {Application} to {Boosting}},
	volume = {55},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	language = {en},
	number = {1},
	urldate = {2021-12-20},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	month = aug,
	year = {1997},
	pages = {119--139},
}

@article{freund_decision-theoretic_1997-1,
	title = {A {Decision}-{Theoretic} {Generalization} of {On}-{Line} {Learning} and an {Application} to {Boosting}},
	volume = {55},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	language = {en},
	number = {1},
	urldate = {2021-12-20},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	month = aug,
	year = {1997},
	pages = {119--139},
}

@article{meir_introduction_2003-1,
	title = {An introduction to boosting and leveraging},
	volume = {2600},
	issn = {0302-9743},
	doi = {10.1007/3-540-36434-x_4},
	abstract = {We provide an introduction to theoretical and practical aspects of Boosting and Ensemble learning, providing a useful reference for researchers in the field of Boosting as well as for those seeking to enter this fascinating area of research. We begin with a short background concerning the necessary learning theoretical foundations of weak learners ahd their linear combinations. We then point out the useful connection between Boosting and the Theory of Optimization, which facilitates the understanding of Boosting and later on enables us to move on to new Boosting algorithms, applicable to a broad spectrum of problems. In order to increase the relevance of the paper to practitioners, we have added remarks, pseudo code, "tricks of the trade", and algorithmic considerations where appropriate. Finally, we illustrate the usefulness of Boosting algorithms by giving an overview of some existing applications. The main ideas are illustrated on the problem of binary classification, although several extensions are discussed. © Springer-Verlag Berlin Heidelberg 2003.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Meir, R. and Rätsch, G.},
	year = {2003},
	note = {ISBN: 9783540005292},
	pages = {118--183},
	annote = {Cited By :241},
	file = {Meir_Rätsch_2003_An introduction to boosting and leveraging.pdf:/home/phyl/Zotero/storage/WFT3GD8W/Meir_Rätsch_2003_An introduction to boosting and leveraging.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/ZCXKBM82/display.html:text/html},
}

@article{viola_robust_2004,
	title = {Robust {Real}-{Time} {Face} {Detection}},
	volume = {57},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/B:VISI.0000013087.49260.fb},
	doi = {10.1023/B:VISI.0000013087.49260.fb},
	abstract = {This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
	language = {en},
	number = {2},
	urldate = {2021-12-20},
	journal = {International Journal of Computer Vision},
	author = {Viola, Paul and Jones, Michael J.},
	month = may,
	year = {2004},
	pages = {137--154},
	file = {Viola_Jones_2004_Robust Real-Time Face Detection.pdf:/home/phyl/Zotero/storage/Z9NY9W6J/Viola_Jones_2004_Robust Real-Time Face Detection.pdf:application/pdf},
}

@inproceedings{lienhart_extended_2002,
	title = {An extended set of haar-like features for rapid object detection},
	volume = {1},
	booktitle = {Proceedings. international conference on image processing},
	publisher = {IEEE},
	author = {Lienhart, Rainer and Maydt, Jochen},
	year = {2002},
	pages = {I--I},
	file = {Lienhart_Maydt_2002_An extended set of haar-like features for rapid object detection.pdf:/home/phyl/Zotero/storage/6UG9MRIZ/Lienhart_Maydt_2002_An extended set of haar-like features for rapid object detection.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/4VU7KUA9/1038171.html:text/html},
}

@misc{noauthor_dimensions_nodate,
	title = {Dimensions},
	url = {https://app.dimensions.ai/discover/publication},
	abstract = {Re-imagining discovery and access to research: grants, datasets, publications, citations, clinical trials, patents and policy documents in one place. With more than 100 million publications and 1 billion citations freely available for personal use, Dimensions provides students and researchers access to the data and information they need - with the lowest barriers possible.},
	language = {en},
	urldate = {2021-12-21},
	file = {Snapshot:/home/phyl/Zotero/storage/R7TRC8CC/publication.html:text/html},
}

@misc{noauthor_deeplearningai_nodate,
	title = {Deeplearning.ai},
	url = {https://www.deeplearning.ai/},
	abstract = {Build your AI career with DeepLearning.AI! Gain world-class education to expand your technical knowledge, get hands-on training to acquire practical skills, and learn from a collaborative community of peers and mentors.},
	language = {en},
	urldate = {2021-12-21},
	journal = {DeepLearning.AI},
	file = {Snapshot:/home/phyl/Zotero/storage/YS5SCG3R/www.deeplearning.ai.html:text/html},
}

@misc{noauthor_autonomous_nodate,
	title = {Autonomous {Mobile} {Robots} {Company} - {AMR} {\textbar} {Robotnik}®},
	url = {https://robotnik.eu/},
	abstract = {Robotnik® - International mobile robotics company specialized in automation solutions with Autonomous mobile robots. Industrial Robot Manufacturers},
	language = {en-US},
	urldate = {2021-12-22},
	journal = {Robotnik},
	file = {Snapshot:/home/phyl/Zotero/storage/8XBJ94JI/robotnik.eu.html:text/html},
}

@article{deng_retinaface_2019,
	title = {Retinaface: {Single}-stage dense face localisation in the wild},
	shorttitle = {Retinaface},
	journal = {arXiv preprint arXiv:1905.00641},
	author = {Deng, Jiankang and Guo, Jia and Zhou, Yuxiang and Yu, Jinke and Kotsia, Irene and Zafeiriou, Stefanos},
	year = {2019},
	file = {Deng et al_2019_Retinaface.pdf:/home/phyl/Zotero/storage/7GKWXMTF/Deng et al_2019_Retinaface.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/IWUG3LAE/1905.html:text/html},
}

@inproceedings{viola_rapid_2001,
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	booktitle = {Proceedings of the 2001 {IEEE} computer society conference on computer vision and pattern recognition. {CVPR} 2001},
	publisher = {Ieee},
	author = {Viola, Paul and Jones, Michael},
	year = {2001},
	pages = {I--I},
	file = {Viola_Jones_2001_Rapid object detection using a boosted cascade of simple features.pdf:/home/phyl/Zotero/storage/6SXXGIJR/Viola_Jones_2001_Rapid object detection using a boosted cascade of simple features.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/VKVG5MVD/990517.html:text/html},
}

@misc{noauthor_face_detector_nodate,
	title = {face\_detector - {ROS} {Wiki}},
	url = {http://wiki.ros.org/face_detector},
	urldate = {2021-12-27},
	file = {face_detector - ROS Wiki:/home/phyl/Zotero/storage/776MLCTT/face_detector.html:text/html},
}

@misc{noauthor_opencvopencv_2021,
	title = {opencv/opencv},
	url = {https://github.com/opencv/opencv},
	abstract = {Open Source Computer Vision Library},
	urldate = {2021-12-27},
	publisher = {OpenCV},
	month = dec,
	year = {2021},
	note = {original-date: 2012-07-19T09:40:17Z},
	keywords = {c-plus-plus, computer-vision, deep-learning, image-processing, opencv},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2021-12-27},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {He et al_2015_Deep Residual Learning for Image Recognition.pdf:/home/phyl/Zotero/storage/ZC956G3C/He et al_2015_Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/phyl/Zotero/storage/V8WHTVKM/1512.html:text/html},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2021-12-27},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:/home/phyl/Zotero/storage/M8R2PQGL/Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/phyl/Zotero/storage/5R4KIG9L/1409.html:text/html},
}

@inproceedings{lin_feature_2017,
	address = {Honolulu, HI},
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099589/},
	doi = {10.1109/CVPR.2017.106},
	urldate = {2021-12-27},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = jul,
	year = {2017},
	pages = {936--944},
	file = {Lin et al_2017_Feature Pyramid Networks for Object Detection.pdf:/home/phyl/Zotero/storage/JM8XIBJ2/Lin et al_2017_Feature Pyramid Networks for Object Detection.pdf:application/pdf;Submitted Version:/home/phyl/Zotero/storage/BH59BXUI/Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf:application/pdf},
}

@inproceedings{shang_understanding_2016,
	title = {Understanding and improving convolutional neural networks via concatenated rectified linear units},
	booktitle = {international conference on machine learning},
	publisher = {PMLR},
	author = {Shang, Wenling and Sohn, Kihyuk and Almeida, Diogo and Lee, Honglak},
	year = {2016},
	pages = {2217--2225},
	file = {Shang et al_2016_Understanding and improving convolutional neural networks via concatenated.pdf:/home/phyl/Zotero/storage/97PHFAF7/Shang et al_2016_Understanding and improving convolutional neural networks via concatenated.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/FYBXL2VZ/shang16.html:text/html},
}

@inproceedings{szegedy_going_2015,
	address = {Boston, MA, USA},
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	urldate = {2021-12-27},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	pages = {1--9},
	file = {Szegedy et al_2015_Going deeper with convolutions.pdf:/home/phyl/Zotero/storage/9CT5GNQI/Szegedy et al_2015_Going deeper with convolutions.pdf:application/pdf;Submitted Version:/home/phyl/Zotero/storage/PK2ZCW2C/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:application/pdf},
}

@inproceedings{lin_feature_2017-1,
	address = {Honolulu, HI},
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099589/},
	doi = {10.1109/CVPR.2017.106},
	urldate = {2021-12-27},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = jul,
	year = {2017},
	pages = {936--944},
	file = {Lin et al_2017_Feature Pyramid Networks for Object Detection.pdf:/home/phyl/Zotero/storage/BFN5R5SF/Lin et al_2017_Feature Pyramid Networks for Object Detection.pdf:application/pdf;Submitted Version:/home/phyl/Zotero/storage/SEERJUTJ/Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf:application/pdf},
}

@misc{zhang_faceboxes_2021,
	title = {{FaceBoxes}: {A} {CPU} {Real}-time {Face} {Detector} with {High} {Accuracy}},
	copyright = {Apache-2.0},
	shorttitle = {{FaceBoxes}},
	url = {https://github.com/sfzhang15/FaceBoxes},
	abstract = {FaceBoxes: A CPU Real-time Face Detector with High Accuracy, IJCB, 2017},
	urldate = {2021-12-27},
	author = {Zhang, Shifeng},
	month = dec,
	year = {2021},
	note = {original-date: 2018-12-25T02:43:50Z},
}

@misc{wong_faceboxes_2021,
	title = {{FaceBoxes} in {PyTorch}},
	copyright = {MIT},
	url = {https://github.com/zisianw/FaceBoxes.PyTorch},
	abstract = {A PyTorch Implementation of FaceBoxes},
	urldate = {2021-12-27},
	author = {Wong, Zi Sian},
	month = dec,
	year = {2021},
	note = {original-date: 2019-01-10T08:04:39Z},
	keywords = {face-detection, faceboxes, pytorch},
}

@article{bazarevsky_blazeface_2019,
	title = {{BlazeFace}: {Sub}-millisecond {Neural} {Face} {Detection} on {Mobile} {GPUs}},
	shorttitle = {{BlazeFace}},
	url = {http://arxiv.org/abs/1907.05047},
	abstract = {We present BlazeFace, a lightweight and well-performing face detector tailored for mobile GPU inference. It runs at a speed of 200-1000+ FPS on flagship devices. This super-realtime performance enables it to be applied to any augmented reality pipeline that requires an accurate facial region of interest as an input for task-specific models, such as 2D/3D facial keypoint or geometry estimation, facial features or expression classification, and face region segmentation. Our contributions include a lightweight feature extraction network inspired by, but distinct from MobileNetV1/V2, a GPU-friendly anchor scheme modified from Single Shot MultiBox Detector (SSD), and an improved tie resolution strategy alternative to non-maximum suppression.},
	urldate = {2021-12-27},
	journal = {arXiv:1907.05047 [cs]},
	author = {Bazarevsky, Valentin and Kartynnik, Yury and Vakunov, Andrey and Raveendran, Karthik and Grundmann, Matthias},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.05047},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 4 pages, 3 figures; CVPR Workshop on Computer Vision for Augmented and Virtual Reality, Long Beach, CA, USA, 2019},
	file = {Bazarevsky et al_2019_BlazeFace.pdf:/home/phyl/Zotero/storage/ZE3TIIQ3/Bazarevsky et al_2019_BlazeFace.pdf:application/pdf;arXiv.org Snapshot:/home/phyl/Zotero/storage/YI2TN27S/1907.html:text/html},
}

@article{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	urldate = {2021-12-27},
	journal = {arXiv:1704.04861 [cs]},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Howard et al_2017_MobileNets.pdf:/home/phyl/Zotero/storage/IXXM5K7P/Howard et al_2017_MobileNets.pdf:application/pdf;arXiv.org Snapshot:/home/phyl/Zotero/storage/EKGHPICF/1704.html:text/html},
}

@article{sandler_mobilenetv2_2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	urldate = {2021-12-27},
	journal = {arXiv:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	note = {arXiv: 1801.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Sandler et al_2019_MobileNetV2.pdf:/home/phyl/Zotero/storage/VBCQMZ8B/Sandler et al_2019_MobileNetV2.pdf:application/pdf;arXiv.org Snapshot:/home/phyl/Zotero/storage/937PGQQU/1801.html:text/html},
}

@misc{noauthor_mediapipe_nodate,
	title = {{MediaPipe}},
	url = {https://google.github.io/mediapipe/},
	abstract = {Cross-platform, customizable ML solutions for live and streaming media.},
	language = {en-US},
	urldate = {2021-12-27},
	journal = {mediapipe},
	file = {Snapshot:/home/phyl/Zotero/storage/MHBXHRGR/mediapipe.html:text/html},
}

@article{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2021-12-27},
	journal = {arXiv:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv: 1708.02002},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Lin et al_2018_Focal Loss for Dense Object Detection.pdf:/home/phyl/Zotero/storage/INYMGTKG/Lin et al_2018_Focal Loss for Dense Object Detection.pdf:application/pdf;arXiv.org Snapshot:/home/phyl/Zotero/storage/VPMH3G3I/1708.html:text/html},
}

@article{najibi_ssh_2017,
	title = {{SSH}: {Single} {Stage} {Headless} {Face} {Detector}},
	shorttitle = {{SSH}},
	url = {http://arxiv.org/abs/1708.03979},
	abstract = {We introduce the Single Stage Headless (SSH) face detector. Unlike two stage proposal-classification detectors, SSH detects faces in a single stage directly from the early convolutional layers in a classification network. SSH is headless. That is, it is able to achieve state-of-the-art results while removing the "head" of its underlying classification network -- i.e. all fully connected layers in the VGG-16 which contains a large number of parameters. Additionally, instead of relying on an image pyramid to detect faces with various scales, SSH is scale-invariant by design. We simultaneously detect faces with different scales in a single forward pass of the network, but from different layers. These properties make SSH fast and light-weight. Surprisingly, with a headless VGG-16, SSH beats the ResNet-101-based state-of-the-art on the WIDER dataset. Even though, unlike the current state-of-the-art, SSH does not use an image pyramid and is 5X faster. Moreover, if an image pyramid is deployed, our light-weight network achieves state-of-the-art on all subsets of the WIDER dataset, improving the AP by 2.5\%. SSH also reaches state-of-the-art results on the FDDB and Pascal-Faces datasets while using a small input size, leading to a runtime of 50 ms/image on a GPU. The code is available at https://github.com/mahyarnajibi/SSH.},
	urldate = {2021-12-27},
	journal = {arXiv:1708.03979 [cs]},
	author = {Najibi, Mahyar and Samangouei, Pouya and Chellappa, Rama and Davis, Larry},
	month = oct,
	year = {2017},
	note = {arXiv: 1708.03979},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: International Conference on Computer Vision (ICCV) 2017},
	file = {Najibi et al_2017_SSH.pdf:/home/phyl/Zotero/storage/NFM5A8U9/Najibi et al_2017_SSH.pdf:application/pdf;arXiv.org Snapshot:/home/phyl/Zotero/storage/3KF2P6N6/1708.html:text/html},
}

@article{tang_pyramidbox_2018,
	title = {{PyramidBox}: {A} {Context}-assisted {Single} {Shot} {Face} {Detector}},
	shorttitle = {{PyramidBox}},
	url = {http://arxiv.org/abs/1803.07737},
	abstract = {Face detection has been well studied for many years and one of remaining challenges is to detect small, blurred and partially occluded faces in uncontrolled environment. This paper proposes a novel context-assisted single shot face detector, named {\textbackslash}emph\{PyramidBox\} to handle the hard face detection problem. Observing the importance of the context, we improve the utilization of contextual information in the following three aspects. First, we design a novel context anchor to supervise high-level contextual feature learning by a semi-supervised method, which we call it PyramidAnchors. Second, we propose the Low-level Feature Pyramid Network to combine adequate high-level context semantic feature and Low-level facial feature together, which also allows the PyramidBox to predict faces of all scales in a single shot. Third, we introduce a context-sensitive structure to increase the capacity of prediction network to improve the final accuracy of output. In addition, we use the method of Data-anchor-sampling to augment the training samples across different scales, which increases the diversity of training data for smaller faces. By exploiting the value of context, PyramidBox achieves superior performance among the state-of-the-art over the two common face detection benchmarks, FDDB and WIDER FACE. Our code is available in PaddlePaddle: {\textbackslash}href\{https://github.com/PaddlePaddle/models/tree/develop/fluid/face\_detection\}\{{\textbackslash}url\{https://github.com/PaddlePaddle/models/tree/develop/fluid/face\_detection\}\}.},
	urldate = {2021-12-27},
	journal = {arXiv:1803.07737 [cs]},
	author = {Tang, Xu and Du, Daniel K. and He, Zeqiang and Liu, Jingtuo},
	month = aug,
	year = {2018},
	note = {arXiv: 1803.07737},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 21 pages, 12 figures},
	file = {Tang et al_2018_PyramidBox.pdf:/home/phyl/Zotero/storage/YI4YRRFN/Tang et al_2018_PyramidBox.pdf:application/pdf;arXiv.org Snapshot:/home/phyl/Zotero/storage/8FUSWZ6I/1803.html:text/html},
}

@article{loy_wider_2019,
	title = {{WIDER} {Face} and {Pedestrian} {Challenge} 2018: {Methods} and {Results}},
	shorttitle = {{WIDER} {Face} and {Pedestrian} {Challenge} 2018},
	url = {http://arxiv.org/abs/1902.06854},
	abstract = {This paper presents a review of the 2018 WIDER Challenge on Face and Pedestrian. The challenge focuses on the problem of precise localization of human faces and bodies, and accurate association of identities. It comprises of three tracks: (i) WIDER Face which aims at soliciting new approaches to advance the state-of-the-art in face detection, (ii) WIDER Pedestrian which aims to find effective and efficient approaches to address the problem of pedestrian detection in unconstrained environments, and (iii) WIDER Person Search which presents an exciting challenge of searching persons across 192 movies. In total, 73 teams made valid submissions to the challenge tracks. We summarize the winning solutions for all three tracks. and present discussions on open problems and potential research directions in these topics.},
	urldate = {2021-12-27},
	journal = {arXiv:1902.06854 [cs]},
	author = {Loy, Chen Change and Lin, Dahua and Ouyang, Wanli and Xiong, Yuanjun and Yang, Shuo and Huang, Qingqiu and Zhou, Dongzhan and Xia, Wei and Li, Quanquan and Luo, Ping and Yan, Junjie and Wang, Jianfeng and Li, Zuoxin and Yuan, Ye and Li, Boxun and Shao, Shuai and Yu, Gang and Wei, Fangyun and Ming, Xiang and Chen, Dong and Zhang, Shifeng and Chi, Cheng and Lei, Zhen and Li, Stan Z. and Zhang, Hongkai and Ma, Bingpeng and Chang, Hong and Shan, Shiguang and Chen, Xilin and Liu, Wu and Zhou, Boyan and Li, Huaxiong and Cheng, Peng and Mei, Tao and Kukharenko, Artem and Vasenin, Artem and Sergievskiy, Nikolay and Yang, Hua and Li, Liangqi and Xu, Qiling and Hong, Yuan and Chen, Lin and Sun, Mingjun and Mao, Yirong and Luo, Shiying and Li, Yongjun and Wang, Ruiping and Xie, Qiaokang and Wu, Ziyang and Lu, Lei and Liu, Yiheng and Zhou, Wengang},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06854},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Report of ECCV 2018 workshop: WIDER Face and Pedestrian Challenge},
	file = {Loy et al_2019_WIDER Face and Pedestrian Challenge 2018.pdf:/home/phyl/Zotero/storage/PVBURNTR/Loy et al_2019_WIDER Face and Pedestrian Challenge 2018.pdf:application/pdf;arXiv.org Snapshot:/home/phyl/Zotero/storage/SBVWYYYN/1902.html:text/html},
}

@misc{biubug6_retinaface-pytorch_2021,
	title = {{RetinaFace}-{PyTorch}},
	copyright = {MIT},
	url = {https://github.com/biubug6/Pytorch_Retinaface},
	abstract = {Retinaface get 80.99\% in widerface hard val using mobilenet0.25.},
	urldate = {2021-12-27},
	author = {biubug6},
	month = dec,
	year = {2021},
	note = {original-date: 2019-09-15T15:09:04Z},
}

@misc{stan_btd_retinaface-tf2_2021,
	title = {{RetinaFace}-tf2},
	copyright = {MIT},
	url = {https://github.com/StanislasBertrand/RetinaFace-tf2},
	abstract = {RetinaFace (RetinaFace: Single-stage Dense Face Localisation in the Wild, published in 2019) reimplemented in Tensorflow 2.0, with pretrained weights available !},
	urldate = {2021-12-27},
	author = {stan\_btd},
	month = dec,
	year = {2021},
	note = {original-date: 2020-07-07T18:05:54Z},
	keywords = {deep-learning, face-detection, insightface, retinaface, tensorflow, tensorflow2, tf2},
}

@misc{noauthor_insightface_nodate,
	title = {{InsightFace}},
	url = {https://insightface.ai/},
	abstract = {InsightFace: an open source 2D\&3D deep face analysis library},
	language = {en},
	urldate = {2021-12-27},
	file = {Snapshot:/home/phyl/Zotero/storage/GLBX3MLB/insightface.ai.html:text/html},
}

@misc{noauthor_insightface-github_2021,
	title = {{InsightFace}-github},
	copyright = {MIT},
	shorttitle = {{InsightFace}},
	url = {https://github.com/deepinsight/insightface},
	abstract = {State-of-the-art 2D and 3D Face Analysis Project},
	urldate = {2021-12-27},
	publisher = {Deep Insight},
	month = dec,
	year = {2021},
	note = {original-date: 2017-09-01T00:36:51Z},
	keywords = {face-detection, pytorch, retinaface, age-estimation, arcface, face-alignment, face-recognition, mxnet, oneflow, paddlepaddle},
}

@misc{geitgey_face_recognition_2021,
	title = {face\_recognition},
	copyright = {MIT},
	url = {https://github.com/ageitgey/face_recognition},
	abstract = {The world's simplest facial recognition api for Python and the command line},
	urldate = {2021-12-27},
	author = {Geitgey, Adam},
	month = dec,
	year = {2021},
	note = {original-date: 2017-03-03T21:52:39Z},
	keywords = {face-detection, face-recognition, machine-learning, python},
}

@misc{noauthor_ros_nodate,
	title = {{ROS}: {Home}},
	url = {https://www.ros.org/},
	urldate = {2021-12-28},
	file = {ROS\: Home:/home/phyl/Zotero/storage/24UH3NP2/www.ros.org.html:text/html},
}

@misc{noauthor_stanford_nodate,
	title = {Stanford {Personal} {Robotics} {Program}},
	url = {http://personalrobotics.stanford.edu/},
	urldate = {2021-12-28},
	file = {Stanford Personal Robotics Program:/home/phyl/Zotero/storage/B5JLHNHA/personalrobotics.stanford.edu.html:text/html},
}

@misc{noauthor_stair_nodate,
	title = {{STAIR}},
	url = {http://stair.stanford.edu/},
	urldate = {2021-12-28},
	file = {STAIR:/home/phyl/Zotero/storage/MM5JMJG7/stair.stanford.edu.html:text/html},
}

@misc{noauthor_open_nodate,
	title = {Open {Robotics}},
	url = {https://www.openrobotics.org},
	abstract = {We are the home for ROS, Gazebo, and Ignition. We build open source robot software and help others to apply it. We offer research, development, and engineering consulting services to industry and government.},
	language = {en-US},
	urldate = {2021-12-28},
	journal = {Open Robotics},
	file = {Snapshot:/home/phyl/Zotero/storage/NZK6E4VH/www.openrobotics.org.html:text/html},
}

@misc{noauthor_packages_nodate,
	title = {Packages - {ROS} {Wiki}},
	url = {http://wiki.ros.org/Packages},
	urldate = {2021-12-28},
	file = {Packages - ROS Wiki:/home/phyl/Zotero/storage/UF8FK2Q7/Packages.html:text/html},
}

@misc{noauthor_navigation_nodate,
	title = {navigation - {ROS} {Wiki}},
	url = {http://wiki.ros.org/navigation},
	urldate = {2021-12-28},
	file = {navigation - ROS Wiki:/home/phyl/Zotero/storage/9PTM4AMK/navigation.html:text/html},
}

@misc{noauthor_git_nodate,
	title = {Git},
	url = {https://git-scm.com/},
	urldate = {2021-12-28},
	file = {Git:/home/phyl/Zotero/storage/J5D3P784/git-scm.com.html:text/html},
}

@misc{noauthor_github_nodate,
	title = {Github},
	url = {https://github.com},
	abstract = {GitHub is where people build software. More than 73 million people use GitHub to discover, fork, and contribute to over 200 million projects.},
	language = {en},
	urldate = {2021-12-28},
	journal = {GitHub},
	file = {Snapshot:/home/phyl/Zotero/storage/DN8UK9DU/github.com.html:text/html},
}

@misc{noauthor_ros_comm_nodate,
	title = {ros\_comm - {ROS} {Wiki}},
	url = {http://wiki.ros.org/ros_comm},
	urldate = {2021-12-28},
	file = {ros_comm - ROS Wiki:/home/phyl/Zotero/storage/TI6W9INQ/ros_comm.html:text/html},
}

@misc{noauthor_rostcpros_nodate,
	title = {{ROS}/{TCPROS} - {ROS} {Wiki}},
	url = {http://wiki.ros.org/ROS/TCPROS},
	urldate = {2021-12-28},
	file = {ROS/TCPROS - ROS Wiki:/home/phyl/Zotero/storage/UJKW7I58/TCPROS.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What is free software and why is it so important for society? — {Free} {Software} {Foundation} — {Working} together for free software},
	url = {https://www.fsf.org/about/what-is-free-software},
	urldate = {2021-12-29},
	file = {What is free software and why is it so important for society? — Free Software Foundation — Working together for free software:/home/phyl/Zotero/storage/JUFSTC5J/what-is-free-software.html:text/html},
}

@misc{noauthor_stack_nodate,
	title = {Stack {Overflow} - {Where} {Developers} {Learn}, {Share}, \& {Build} {Careers}},
	url = {https://stackoverflow.com/},
	abstract = {Stack Overflow is the largest, most trusted online community for developers to learn, share​ ​their programming ​knowledge, and build their careers.},
	urldate = {2021-12-29},
	journal = {Stack Overflow},
	file = {Snapshot:/home/phyl/Zotero/storage/I9EEVEL3/stackoverflow.com.html:text/html},
}

@misc{noauthor_ros-wiki_nodate,
	title = {{ROS}-{Wiki}},
	url = {http://wiki.ros.org/},
	urldate = {2021-12-29},
	file = {Documentation - ROS Wiki:/home/phyl/Zotero/storage/TKSXITL2/wiki.ros.org.html:text/html},
}

@misc{noauthor_ros-discourse_nodate,
	title = {{ROS}-{Discourse}},
	url = {https://discourse.ros.org/},
	abstract = {The ROS discussion forum},
	language = {en},
	urldate = {2021-12-29},
	journal = {ROS Discourse},
	file = {Snapshot:/home/phyl/Zotero/storage/EWWQUMPU/discourse.ros.org.html:text/html},
}

@misc{noauthor_ros-answers_nodate,
	title = {{ROS}-{Answers}: {Open} {Source} {Q}\&{A} {Forum}},
	url = {https://answers.ros.org/questions/},
	urldate = {2021-12-29},
	file = {Questions - ROS Answers\: Open Source Q&A Forum:/home/phyl/Zotero/storage/K4FQRYLN/questions.html:text/html},
}

@inproceedings{dalal_histograms_2005,
	address = {San Diego, CA, USA},
	title = {Histograms of {Oriented} {Gradients} for {Human} {Detection}},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467360/},
	doi = {10.1109/CVPR.2005.177},
	urldate = {2021-12-29},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	publisher = {IEEE},
	author = {Dalal, N. and Triggs, B.},
	year = {2005},
	pages = {886--893},
	file = {Dalal_Triggs_2005_Histograms of Oriented Gradients for Human Detection.pdf:/home/phyl/Zotero/storage/2ZTUZYZ5/Dalal_Triggs_2005_Histograms of Oriented Gradients for Human Detection.pdf:application/pdf;Submitted Version:/home/phyl/Zotero/storage/TRXPQB7J/Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detecti.pdf:application/pdf},
}

@patent{mcconnell_method_1986,
	title = {Method of and apparatus for pattern recognition},
	url = {https://patents.google.com/patent/US4567610/en},
	nationality = {US},
	assignee = {WAYLAND RES Inc},
	number = {US4567610A},
	urldate = {2021-12-29},
	author = {McConnell, Robert K.},
	month = jan,
	year = {1986},
	keywords = {histogram, ith symbol, terms, test, values},
	file = {McConnell_1986_Method of and apparatus for pattern recognition.pdf:/home/phyl/Zotero/storage/A7K8HUNU/McConnell_1986_Method of and apparatus for pattern recognition.pdf:application/pdf},
}

@techreport{william_t_freeman_orientation_1994,
	address = {Cambridge, MA 02139},
	title = {Orientation {Histograms} for {Hand} {Gesture} {Recognition}},
	url = {https://www.merl.com/publications/TR94-03/},
	number = {TR94-03},
	institution = {MERL - Mitsubishi Electric Research Laboratories},
	author = {William T. Freeman, Michal Roth},
	month = dec,
	year = {1994},
	file = {Freeman and Roth - Orientation Histograms for Hand Gesture Recognitio.pdf:/home/phyl/Zotero/storage/QBPLT3PM/Freeman and Roth - Orientation Histograms for Hand Gesture Recognitio.pdf:application/pdf},
}

@inproceedings{salhi_histograms_2013,
	address = {Sousse},
	title = {Histograms of fuzzy oriented gradients for face recognition},
	isbn = {978-1-4673-5285-7 978-1-4673-5284-0 978-1-4673-5283-3},
	url = {http://ieeexplore.ieee.org/document/6522006/},
	doi = {10.1109/ICCAT.2013.6522006},
	urldate = {2021-12-29},
	booktitle = {2013 {International} {Conference} on {Computer} {Applications} {Technology} ({ICCAT})},
	publisher = {IEEE},
	author = {Salhi, A. I. and Kardouchi, M. and Belacel, N.},
	month = jan,
	year = {2013},
	pages = {1--5},
	file = {Salhi et al_2013_Histograms of fuzzy oriented gradients for face recognition.pdf:/home/phyl/Zotero/storage/6PX8WP6H/Salhi et al_2013_Histograms of fuzzy oriented gradients for face recognition.pdf:application/pdf},
}

@inproceedings{huang_labeled_2008,
	title = {Labeled faces in the wild: {A} database forstudying face recognition in unconstrained environments},
	shorttitle = {Labeled faces in the wild},
	booktitle = {Workshop on faces in'{Real}-{Life}'{Images}: detection, alignment, and recognition},
	author = {Huang, Gary B. and Mattar, Marwan and Berg, Tamara and Learned-Miller, Eric},
	year = {2008},
	file = {Huang et al_2008_Labeled faces in the wild.pdf:/home/phyl/Zotero/storage/HJYKJ78W/Huang et al_2008_Labeled faces in the wild.pdf:application/pdf;Snapshot:/home/phyl/Zotero/storage/MDRZWTP2/inria-00321923.html:text/html},
}

@inproceedings{bo_wu_fast_2004,
	address = {Seoul, Korea},
	title = {Fast rotation invariant multi-view face detection based on real adaboost},
	isbn = {978-0-7695-2122-0},
	url = {http://ieeexplore.ieee.org/document/1301512/},
	doi = {10.1109/AFGR.2004.1301512},
	urldate = {2021-12-30},
	booktitle = {Sixth {IEEE} {International} {Conference} on {Automatic} {Face} and {Gesture} {Recognition}, 2004. {Proceedings}.},
	publisher = {IEEE},
	author = {{Bo Wu} and {Haizhou Ai} and {Chang Huang} and {Shihong Lao}},
	year = {2004},
	pages = {79--84},
	file = {Bo Wu et al_2004_Fast rotation invariant multi-view face detection based on real adaboost.pdf:/home/phyl/Zotero/storage/JMCKVBG9/Bo Wu et al_2004_Fast rotation invariant multi-view face detection based on real adaboost.pdf:application/pdf},
}

@incollection{goos_statistical_2002,
	address = {Berlin, Heidelberg},
	title = {Statistical {Learning} of {Multi}-view {Face} {Detection}},
	volume = {2353},
	isbn = {978-3-540-43748-2 978-3-540-47979-6},
	url = {http://link.springer.com/10.1007/3-540-47979-1_5},
	language = {en},
	urldate = {2021-12-30},
	booktitle = {Computer {Vision} — {ECCV} 2002},
	publisher = {Springer Berlin Heidelberg},
	author = {Li, Stan Z. and Zhu, Long and Zhang, ZhenQiu and Blake, Andrew and Zhang, HongJiang and Shum, Harry},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Heyden, Anders and Sparr, Gunnar and Nielsen, Mads and Johansen, Peter},
	year = {2002},
	doi = {10.1007/3-540-47979-1_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {67--81},
	file = {Statistical Learning of Multi-view Face Detection:/home/phyl/Zotero/storage/8W9T45GA/3-540-47979-1_5.pdf:application/pdf},
}

@misc{noauthor_flickr_nodate,
	title = {Flickr},
	url = {https://identity.flickr.com/},
	urldate = {2021-12-30},
	file = {Flickr Login:/home/phyl/Zotero/storage/DCYRLXP2/sign-up.html:text/html},
}

@article{kornel_az_2015,
	title = {Az arcfelismerési zavarok osztályozása a kialakulás oka, az idegtudományi, valamint a viselkedéses eredmények tükrében},
	volume = {70},
	issn = {0025-0279, 1588-2799},
	url = {https://akjournals.com/view/journals/0016/70/3/article-p507.xml},
	doi = {10.1556/0016.2015.70.3.2},
	abstract = {A prosopagnosia az ismerős arcok felismerési zavara normál látási és kognitív funkciók ellenére. A zavar szerzett formája az occipito-temporalis területek károsodásának következtében alakul ki, míg a születéstől meglévő zavar idegrendszeri háttere nagymértékű heterogenitást mutat. Az agykárosodási mintázat változatossága mellett a tünetek súlyosságában is nagy egyéni különbségeket találunk. Bár az elmúlt 20 évben az arcfelismerési zavar kutatását egyre nagyobb érdeklődés övezi, a vizsgálati eredmények alapján levonható következtetések általánosíthatóságát limitálja, hogy ezek többnyire egyedi, elszigetelt esetekből indulnak ki. Ha egyértelmű konklúzió nem rajzolódik is ki az ismert eredmények alapján, az mindenképpen megállapítható, hogy számos aspektus tekintetében a zavar alcsoportokra osztható. A tárgyfelismerési zavarok osztályozásának analógiájára felmerül az arcfelismerési zavarok apperceptív és asszociatív alcsoportokra bontásának lehetősége, valamint hogy a zavar akár spektrum jellegű is lehet. A prosopagnosia veleszületett típusában a generációkon át megjelenő esetek ráirányítják a figyelmet a genetikai eredet kérdésére. Jelen tanulmányban áttekintjük az arcfelismerési zavar csoportosításának lehetséges szempontjait. Továbbá az ismert esetek áttekintésével próbáljuk megmutatni a differenciálás szükségességét, valamint rávilágítunk azokra a szempontokra, amelyek szem előtt tartásával a következő vizsgálatok során érdemi lépéseket tehetünk a zavar megismerése felé.},
	number = {3},
	urldate = {2022-01-01},
	journal = {Magyar Pszichológiai Szemle},
	author = {Kornél, Németh and Márta, Zimmer},
	month = sep,
	year = {2015},
	pages = {507--535},
}

@misc{noauthor_oracle_nodate,
	title = {Oracle {VM} {VirtualBox}},
	url = {https://www.virtualbox.org/},
	urldate = {2022-01-02},
	file = {Oracle VM VirtualBox:/home/phyl/Zotero/storage/P3Q6C4E6/www.virtualbox.org.html:text/html},
}

@misc{noauthor_gazebo_nodate,
	title = {Gazebo},
	url = {http://gazebosim.org/},
	urldate = {2022-01-02},
	file = {Gazebo:/home/phyl/Zotero/storage/6J3EDQWY/gazebosim.org.html:text/html},
}

@misc{noauthor_rviz_nodate,
	title = {rviz - {ROS} {Wiki}},
	url = {http://wiki.ros.org/rviz},
	urldate = {2022-01-02},
	file = {rviz - ROS Wiki:/home/phyl/Zotero/storage/TQEGTVMB/rviz.html:text/html},
}

@misc{noauthor_docker_nodate,
	title = {Docker},
	url = {https://www.docker.com/},
	abstract = {Learn how Docker helps developers bring their ideas to life by conquering the complexity of app development.},
	language = {en},
	urldate = {2022-01-02},
	file = {Snapshot:/home/phyl/Zotero/storage/V36B3BNJ/www.docker.com.html:text/html},
}

@misc{noauthor_singularityce_nodate,
	title = {{SingularityCE}},
	url = {https://sylabs.io/singularity},
	abstract = {The container runtime that combines high performance and ease of use},
	urldate = {2022-01-02},
	journal = {Sylabs.io},
	file = {Snapshot:/home/phyl/Zotero/storage/TVHE8M25/singularity.html:text/html},
}

@misc{craigloewen-msft_wsl_nodate,
	title = {{WSL} - {Windows} {Subsystem} for {Linux} {Documentation}},
	url = {https://docs.microsoft.com/en-us/windows/wsl/},
	abstract = {Overview of the Windows Subsystem for Linux documentation.},
	language = {en-us},
	urldate = {2022-01-02},
	author = {craigloewen-msft},
	file = {Snapshot:/home/phyl/Zotero/storage/8USFYQIW/wsl.html:text/html},
}

@misc{noauthor_xacro_nodate,
	title = {xacro - {ROS} {Wiki}},
	url = {http://wiki.ros.org/xacro},
	urldate = {2022-01-02},
	file = {xacro - ROS Wiki:/home/phyl/Zotero/storage/XEZVWRC6/xacro.html:text/html},
}
